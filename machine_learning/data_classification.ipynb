{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adicionando o caminho do projeto ao path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/edithvidal/Documents/Github/data_science_analysis/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instalando sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./venv/lib/python3.13/site-packages (from scikit-learn) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./venv/lib/python3.13/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: category_encoders in ./venv/lib/python3.13/site-packages (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in ./venv/lib/python3.13/site-packages (from category_encoders) (2.2.2)\n",
      "Requirement already satisfied: pandas>=1.0.5 in ./venv/lib/python3.13/site-packages (from category_encoders) (2.2.3)\n",
      "Requirement already satisfied: patsy>=0.5.1 in ./venv/lib/python3.13/site-packages (from category_encoders) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn>=1.6.0 in ./venv/lib/python3.13/site-packages (from category_encoders) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in ./venv/lib/python3.13/site-packages (from category_encoders) (1.15.2)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in ./venv/lib/python3.13/site-packages (from category_encoders) (0.14.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.13/site-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.13/site-packages (from pandas>=1.0.5->category_encoders) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.13/site-packages (from pandas>=1.0.5->category_encoders) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.13/site-packages (from scikit-learn>=1.6.0->category_encoders) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.13/site-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
      "Requirement already satisfied: packaging>=21.3 in ./venv/lib/python3.13/site-packages (from statsmodels>=0.9.0->category_encoders) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: imbalanced-learn in ./venv/lib/python3.13/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in ./venv/lib/python3.13/site-packages (from imbalanced-learn) (2.2.2)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in ./venv/lib/python3.13/site-packages (from imbalanced-learn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in ./venv/lib/python3.13/site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in ./venv/lib/python3.13/site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in ./venv/lib/python3.13/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in ./venv/lib/python3.13/site-packages (from imbalanced-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install category_encoders\n",
    "%pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lendo dados da base após análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/3syxl8gd5ks431s8c9n9mrs00000gn/T/ipykernel_2258/2452157731.py:1: DtypeWarning: Columns (0,2,3,4,5,7,8,9,10,11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_accidents = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 467078 entries, 0 to 467077\n",
      "Data columns (total 15 columns):\n",
      " #   Column                  Non-Null Count   Dtype         \n",
      "---  ------                  --------------   -----         \n",
      " 0   tipo_data               429733 non-null  object        \n",
      " 1   mes_ano                 467078 non-null  datetime64[ns]\n",
      " 2   dia_semana              429733 non-null  object        \n",
      " 3   fase_dia                429733 non-null  object        \n",
      " 4   condicao_metereologica  429733 non-null  object        \n",
      " 5   uf                      429733 non-null  object        \n",
      " 6   br                      429733 non-null  float64       \n",
      " 7   uso_solo                429733 non-null  object        \n",
      " 8   tipo_pista              429733 non-null  object        \n",
      " 9   sentido_via             429733 non-null  object        \n",
      " 10  tipo_acidente           429733 non-null  object        \n",
      " 11  causa_acidente          429733 non-null  object        \n",
      " 12  classificacao_acidente  429733 non-null  object        \n",
      " 13  pessoas                 429733 non-null  float64       \n",
      " 14  veiculos                429733 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(3), object(11)\n",
      "memory usage: 53.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_accidents = pd.read_csv(\n",
    "    \"analysis/datatran2018_2024_v3.csv\",\n",
    "    parse_dates=[\"mes_ano\"]\n",
    ")\n",
    "df_accidents.info()\n",
    "df_accidents.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removendo colunas que não serão utilizadas para a classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 429733 entries, 0 to 429732\n",
      "Data columns (total 9 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   dia_semana              429733 non-null  object \n",
      " 1   fase_dia                429733 non-null  object \n",
      " 2   condicao_metereologica  429733 non-null  object \n",
      " 3   br                      429733 non-null  float64\n",
      " 4   uso_solo                429733 non-null  object \n",
      " 5   tipo_pista              429733 non-null  object \n",
      " 6   sentido_via             429733 non-null  object \n",
      " 7   tipo_acidente           429733 non-null  object \n",
      " 8   causa_acidente          429733 non-null  object \n",
      "dtypes: float64(1), object(8)\n",
      "memory usage: 32.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df_accidents.copy()\n",
    "df.drop(columns=[\n",
    "    \"uf\",\n",
    "    \"mes_ano\",\n",
    "    \"tipo_data\",\n",
    "    \"classificacao_acidente\",\n",
    "    \"pessoas\",\n",
    "    \"veiculos\"\n",
    "], inplace=True)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agrupando tipos de acidente para facilitar a classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tipo_acidente\n",
       "Colisão traseira                   81314\n",
       "Saída de leito carroçável          67518\n",
       "Colisão transversal                54276\n",
       "Tombamento                         35660\n",
       "Colisão frontal                    28970\n",
       "Colisão lateral                    24476\n",
       "Colisão lateral mesmo sentido      23272\n",
       "Atropelamento de pedestre          19884\n",
       "Queda de ocupante de veículo       19182\n",
       "Colisão com objeto                 16729\n",
       "Colisão com objeto estático        14144\n",
       "Capotamento                        12119\n",
       "Incêndio                            8092\n",
       "Atropelamento de animal             7337\n",
       "Engavetamento                       6732\n",
       "Colisão lateral sentido oposto      6444\n",
       "Derramamento de carga               1032\n",
       "Eventos atípicos                    1012\n",
       "Colisão com objeto em movimento      941\n",
       "Danos eventuais                      599\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tipo_acidente\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tipo_acidente\n",
       "Colisão                         250566\n",
       "Perda de controle               122029\n",
       "Atropelamento                    27221\n",
       "Queda de ocupante de veículo     19182\n",
       "Evento externo                   10735\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidents = {\n",
    "    \"Colisão traseira\": \"Colisão\",\n",
    "    \"Colisão transversal\": \"Colisão\",\n",
    "    \"Colisão frontal\": \"Colisão\",\n",
    "    \"Colisão lateral\": \"Colisão\",\n",
    "    \"Colisão lateral mesmo sentido\": \"Colisão\",\n",
    "    \"Colisão lateral sentido oposto\": \"Colisão\",\n",
    "    \"Colisão com objeto\": \"Colisão\",\n",
    "    \"Colisão com objeto estático\": \"Colisão\",\n",
    "    \"Colisão com objeto em movimento\": \"Colisão\",\n",
    "    \"Atropelamento de animal\": \"Atropelamento\",\n",
    "    \"Atropelamento de pedestre\": \"Atropelamento\",\n",
    "    \"Capotamento\": \"Perda de controle\",\n",
    "    \"Tombamento\": \"Perda de controle\",\n",
    "    \"Saída de leito carroçável\": \"Perda de controle\",\n",
    "    \"Engavetamento\": \"Perda de controle\",\n",
    "    \"Derramamento de carga\": \"Evento externo\",\n",
    "    \"Incêndio\": \"Evento externo\",\n",
    "    \"Eventos atípicos\": \"Evento externo\",\n",
    "    \"Danos eventuais\": \"Evento externo\",\n",
    "    \"Queda de ocupante de veículo\": \"Queda de ocupante de veículo\"\n",
    "}\n",
    "\n",
    "df[\"tipo_acidente\"] = df[\"tipo_acidente\"].map(accidents)\n",
    "df[\"tipo_acidente\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHotEncoder + RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              precision    recall  f1-score   support\n",
      "\n",
      "               Atropelamento       0.86      0.73      0.79      8166\n",
      "                     Colisão       0.82      0.59      0.69     75170\n",
      "              Evento externo       0.36      0.79      0.49      3220\n",
      "           Perda de controle       0.47      0.65      0.54     36609\n",
      "Queda de ocupante de veículo       0.12      0.19      0.15      5755\n",
      "\n",
      "                    accuracy                           0.60    128920\n",
      "                   macro avg       0.52      0.59      0.53    128920\n",
      "                weighted avg       0.68      0.60      0.62    128920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_col = 'tipo_acidente'\n",
    "\n",
    "# 1. Features categóricas que você quer usar\n",
    "categorical_features = [\n",
    "    'dia_semana', 'fase_dia', 'condicao_metereologica',\n",
    "    'uso_solo', 'tipo_pista', 'sentido_via', 'causa_acidente'\n",
    "]\n",
    "\n",
    "# 2. Separar X e y\n",
    "X = df[categorical_features]\n",
    "y = df[target_col]\n",
    "\n",
    "# 3. Separar treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Preprocessamento (OneHotEncoder para dados categóricos)\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "])\n",
    "\n",
    "# 5. Montar o pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        class_weight='balanced',  # trata desbalanceamento\n",
    "        random_state=42,\n",
    "        n_estimators=30,\n",
    "        max_depth=10,\n",
    "        n_jobs=-1  # usa todos os núcleos disponíveis\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 6. Treinar o modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 7. Avaliar\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar matriz de confusão\n",
    "cm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipeline.classes_)\n",
    "\n",
    "# Visualizar\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp.plot(ax=ax, cmap='Blues', xticks_rotation=45)\n",
    "plt.title('Matriz de Confusão - RandomForest + OneHotEncoder')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TargetEncoder + RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Melhores parâmetros: {'clf__max_depth': 9, 'clf__max_features': 'log2', 'clf__min_samples_leaf': 3, 'clf__min_samples_split': 6, 'clf__n_estimators': 73}\n",
      "Melhor score (f1_macro): 0.5329068990347604\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Variáveis\n",
    "target_col = 'tipo_acidente'\n",
    "categorical_features = [\n",
    "    'dia_semana',\n",
    "    'fase_dia',\n",
    "    'condicao_metereologica',\n",
    "    'uso_solo',\n",
    "    'tipo_pista',\n",
    "    'sentido_via',\n",
    "    'causa_acidente'\n",
    "]\n",
    "\n",
    "# Separar dados\n",
    "X = df[categorical_features]\n",
    "y = df[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline com TargetEncoder + RandomForest\n",
    "pipeline = Pipeline([\n",
    "    ('encoder', TargetEncoder(cols=categorical_features)),\n",
    "    ('clf', RandomForestClassifier(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# Grade de hiperparâmetros para RandomForest\n",
    "param_distributions = {\n",
    "    'clf__n_estimators': randint(50, 100),          # de 50 até 149\n",
    "    'clf__max_depth': randint(5, 10),               # de 5 até 19\n",
    "    'clf__min_samples_split': randint(2, 11),       # de 2 até 10\n",
    "    'clf__min_samples_leaf': randint(1, 5),         # de 1 até 4\n",
    "    'clf__max_features': ['sqrt', 'log2']           # evita 'auto' (depreciado no RF)\n",
    "}\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=2,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Executa busca\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Melhores parâmetros\n",
    "print(\"Melhores parâmetros:\", random_search.best_params_)\n",
    "print(\"Melhor score (f1_macro):\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis\n",
    "target_col = 'tipo_acidente'\n",
    "categorical_features = [\n",
    "    'dia_semana', \n",
    "    'fase_dia', \n",
    "    'condicao_metereologica',\n",
    "    'uso_solo', \n",
    "    'tipo_pista', \n",
    "    'sentido_via', \n",
    "    'causa_acidente'\n",
    "]\n",
    "\n",
    "# Separar dados\n",
    "X = df[categorical_features]\n",
    "y = df[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline com TargetEncoder + RandomForest\n",
    "pipeline = Pipeline([\n",
    "    ('encoder', TargetEncoder(cols=categorical_features)),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        class_weight='balanced',\n",
    "        n_estimators=73,\n",
    "        n_jobs=-1,\n",
    "        max_depth=9,\n",
    "        min_samples_split=6,\n",
    "        min_samples_leaf=3,\n",
    "        max_features=\"log2\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Treinar\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Previsões\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Relatório textual\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar matriz de confusão\n",
    "cm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipeline.classes_)\n",
    "\n",
    "# Visualizar\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp.plot(ax=ax, cmap='Blues', xticks_rotation=45)\n",
    "plt.title('Matriz de Confusão - RandomForest + TargetEncoder')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TargetEncoder + DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "\n",
    "# Variáveis\n",
    "target_col = 'tipo_acidente'\n",
    "categorical_features = [\n",
    "    'dia_semana', \n",
    "    'fase_dia', \n",
    "    'condicao_metereologica',\n",
    "    'uso_solo', \n",
    "    'tipo_pista', \n",
    "    'sentido_via', \n",
    "    'causa_acidente'\n",
    "]\n",
    "\n",
    "# Separar dados\n",
    "X = df[categorical_features]\n",
    "y = df[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('encoder', TargetEncoder(cols=categorical_features)),\n",
    "    ('clf', DecisionTreeClassifier(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# Grade de hiperparâmetros\n",
    "param_grid = {\n",
    "    'clf__max_depth': [None, 5, 10, 15, 20],\n",
    "    'clf__min_samples_split': [2, 5, 10, 20],\n",
    "    'clf__min_samples_leaf': [1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "# Configura o GridSearch\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,               # número de folds\n",
    "    scoring='f1_macro',  # métrica para multiclasse balanceada\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Executa busca\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Melhores parâmetros\n",
    "print(\"Melhores parâmetros:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis\n",
    "target_col = 'tipo_acidente'\n",
    "categorical_features = [\n",
    "    'dia_semana', 'fase_dia', 'condicao_metereologica',\n",
    "    'uf', 'uso_solo', 'tipo_pista', 'sentido_via', 'causa_acidente'\n",
    "]\n",
    "\n",
    "# Separar dados\n",
    "X = df[categorical_features]\n",
    "y = df[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline com hiperparâmetros ajustados\n",
    "pipeline = Pipeline([\n",
    "    ('encoder', TargetEncoder(cols=categorical_features)),\n",
    "    ('classifier', DecisionTreeClassifier(\n",
    "        class_weight='balanced',\n",
    "        max_depth=10,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Treinar modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prever\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Avaliar\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar matriz de confusão\n",
    "cm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipeline.classes_)\n",
    "\n",
    "# Visualizar\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp.plot(ax=ax, cmap='Blues', xticks_rotation=45)\n",
    "plt.title('Matriz de Confusão - RandomForest + TargetEncoder')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders import TargetEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Variáveis\n",
    "target_col = 'tipo_acidente'\n",
    "categorical_features = [\n",
    "    'dia_semana',\n",
    "    'fase_dia',\n",
    "    'condicao_metereologica',\n",
    "    'uso_solo',\n",
    "    'tipo_pista',\n",
    "    'sentido_via',\n",
    "    'causa_acidente'\n",
    "]\n",
    "\n",
    "# Separar dados\n",
    "X = df[categorical_features]\n",
    "y = df[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline com TargetEncoder + XGBClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('encoder', TargetEncoder(cols=categorical_features)),\n",
    "    ('clf', XGBClassifier(\n",
    "        objective='multi:softmax',  # ou 'multi:softprob' para probabilidades\n",
    "        num_class=len(y.unique()),  # obrigatório para classificação multiclasse\n",
    "        eval_metric='mlogloss',\n",
    "        use_label_encoder=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Grade de hiperparâmetros para XGBoost\n",
    "param_distributions = {\n",
    "    'clf__n_estimators': randint(50, 150),\n",
    "    'clf__max_depth': randint(3, 10),\n",
    "    'clf__learning_rate': uniform(0.01, 0.3),\n",
    "    'clf__subsample': uniform(0.6, 0.4),\n",
    "    'clf__colsample_bytree': uniform(0.6, 0.4),\n",
    "    'clf__gamma': uniform(0, 5)\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=2,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Executa busca\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Melhores parâmetros\n",
    "print(\"Melhores parâmetros:\", random_search.best_params_)\n",
    "print(\"Melhor score (f1_macro):\", random_search.best_score_)\n",
    "\n",
    "# Avaliação no teste\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(\"\\nRelatório de classificação:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
